{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "from typing import Dict, Union\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_reference_credibility(url: str) -> Dict[str, Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Basic URL credibility evaluation based on domain type and known reliable sources.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains 'score' (float) and 'explanation' (str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the URL\n",
    "        extracted = tldextract.extract(url)\n",
    "        domain = f\"{extracted.domain}.{extracted.suffix}\"\n",
    "        \n",
    "        # Define trusted domains and their scores\n",
    "        trusted_domains = {\n",
    "            'edu': 0.9,    # Educational institutions\n",
    "            'gov': 0.9,    # Government websites\n",
    "            'org': 0.7,    # Non-profit organizations\n",
    "        }\n",
    "        \n",
    "        # Define trusted sources\n",
    "        trusted_sources = {\n",
    "            'nature.com': 0.9,\n",
    "            'science.org': 0.9,\n",
    "            'scholar.google.com': 0.8,\n",
    "        }\n",
    "        \n",
    "        # Basic scoring logic\n",
    "        score = 0.5  # Default score\n",
    "        explanation = []\n",
    "        \n",
    "        # Check domain type\n",
    "        if extracted.suffix in trusted_domains:\n",
    "            score = trusted_domains[extracted.suffix]\n",
    "            explanation.append(f\"Trusted domain type ({extracted.suffix})\")\n",
    "        \n",
    "        # Check for trusted sources\n",
    "        if domain in trusted_sources:\n",
    "            score = trusted_sources[domain]\n",
    "            explanation.append(f\"Recognized trusted source ({domain})\")\n",
    "            \n",
    "        return {\n",
    "            \"score\": score,\n",
    "            \"explanation\": \" | \".join(explanation) if explanation else \"Basic domain evaluation\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"explanation\": f\"Error evaluating URL: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fact_check(url: str) -> Dict[str, Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Basic fact-checking evaluation based on known fact-checking sources and scientific journals.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains 'fact_check_score' (float) and 'explanation' (str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the URL\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc.lower()\n",
    "        \n",
    "        # Remove 'www.' if present\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "            \n",
    "        # Define fact-checking sources and their scores\n",
    "        fact_check_sources = {\n",
    "            'snopes.com': 0.9,\n",
    "            'factcheck.org': 0.9,\n",
    "            'politifact.com': 0.85,\n",
    "            'reuters.com': 0.85,\n",
    "            'apnews.com': 0.85,\n",
    "            'nature.com': 0.95,       # Peer-reviewed scientific journal\n",
    "            'science.org': 0.95,      # Peer-reviewed scientific journal\n",
    "            'thelancet.com': 0.95,    # Medical journal\n",
    "        }\n",
    "        \n",
    "        # Basic scoring logic\n",
    "        score = 0.5  # Default score\n",
    "        explanation = []\n",
    "        \n",
    "        # Check if it's a known fact-checking or scientific source\n",
    "        for source, source_score in fact_check_sources.items():\n",
    "            if domain == source or domain.endswith('.' + source):\n",
    "                score = source_score\n",
    "                if source in ['nature.com', 'science.org', 'thelancet.com']:\n",
    "                    explanation.append(f\"Peer-reviewed scientific source ({source})\")\n",
    "                else:\n",
    "                    explanation.append(f\"Recognized fact-checking source ({source})\")\n",
    "                break\n",
    "                \n",
    "        # Additional check for scientific article patterns\n",
    "        path = parsed_url.path.lower()\n",
    "        if any(x in path for x in ['/article/', '/research/', '/study/', '/paper/']):\n",
    "            score += 0.1\n",
    "            explanation.append(\"Contains scientific article indicators\")\n",
    "            score = min(score, 1.0)  # Cap at 1.0\n",
    "            \n",
    "        return {\n",
    "            \"fact_check_score\": score,\n",
    "            \"explanation\": \" | \".join(explanation) if explanation else \"Basic fact-check evaluation\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"fact_check_score\": 0.0,\n",
    "            \"explanation\": f\"Error evaluating URL: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_citations(url: str) -> Dict[str, Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Evaluates citation count and reference quality from webpage content.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains 'citation_score' (float) and 'explanation' (str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Initialize counters and score\n",
    "        citation_count = 0\n",
    "        reference_count = 0\n",
    "        score = 0.5  # Default score\n",
    "        explanation = []\n",
    "        \n",
    "        # Find citations in different formats\n",
    "        # Look for <cite> tags\n",
    "        cite_tags = soup.find_all('cite')\n",
    "        citation_count += len(cite_tags)\n",
    "        \n",
    "        # Look for numbered references [1], [2], etc.\n",
    "        numbered_refs = re.findall(r'\\[\\d+\\]', response.text)\n",
    "        citation_count += len(numbered_refs)\n",
    "        \n",
    "        # Look for reference or bibliography section\n",
    "        ref_sections = soup.find_all(['div', 'section'], \n",
    "                                   class_=re.compile(r'reference|bibliography|citations', re.I))\n",
    "        \n",
    "        if ref_sections:\n",
    "            # Count references in these sections\n",
    "            for section in ref_sections:\n",
    "                # Count list items in reference sections\n",
    "                references = section.find_all('li')\n",
    "                reference_count += len(references)\n",
    "        \n",
    "        # Calculate score based on citations\n",
    "        total_citations = max(citation_count, reference_count)\n",
    "        \n",
    "        if total_citations > 0:\n",
    "            # Adjust score based on number of citations\n",
    "            if total_citations >= 50:\n",
    "                score = 0.9\n",
    "            elif total_citations >= 30:\n",
    "                score = 0.8\n",
    "            elif total_citations >= 15:\n",
    "                score = 0.7\n",
    "            elif total_citations >= 5:\n",
    "                score = 0.6\n",
    "            \n",
    "            explanation.append(f\"Found {total_citations} citations/references\")\n",
    "            \n",
    "        # Look for DOI references\n",
    "        doi_refs = re.findall(r'doi\\.org/\\d+\\.\\d+/\\S+', response.text)\n",
    "        if doi_refs:\n",
    "            score += 0.1  # Bonus for having DOI references\n",
    "            explanation.append(f\"Found {len(doi_refs)} DOI references\")\n",
    "            score = min(score, 1.0)  # Cap at 1.0\n",
    "            \n",
    "        return {\n",
    "            \"citation_score\": score,\n",
    "            \"explanation\": \" | \".join(explanation) if explanation else \"No citations found\",\n",
    "            \"citation_count\": total_citations\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"citation_score\": 0.0,\n",
    "            \"explanation\": f\"Error analyzing citations: {str(e)}\",\n",
    "            \"citation_count\": 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9, 'explanation': 'Recognized trusted source (nature.com)'}\n",
      "{'fact_check_score': 0.95, 'explanation': 'Peer-reviewed scientific source (nature.com)'}\n",
      "{'citation_score': 0.6, 'explanation': 'Found 2 citations/references | Found 3 DOI references', 'citation_count': 2}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main method to call evaluate_reference_credibility function and evaluate_reference_credibility and return the scores\n",
    "\"\"\"\n",
    "\n",
    "url = \"https://www.nature.com/articles/s41586-020-2649-2\"\n",
    "result_domain = evaluate_reference_credibility(url)\n",
    "result_fact_check = evaluate_fact_check(url)\n",
    "result_citations = evaluate_citations(url)\n",
    "print(result_domain)\n",
    "print(result_fact_check)\n",
    "print(result_citations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

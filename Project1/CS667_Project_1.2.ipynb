{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "from typing import Dict, Union\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from newsapi import NewsApiClient\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reference_credibility(url: str) -> Dict[str, Union[float, str]]:\n",
    "    base_score = 0.5  # Initialize as float\n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert-tiny-finetuned-fake-news-detection\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/bert-tiny-finetuned-fake-news-detection\")\n",
    "        \n",
    "        # Get some text content from the URL\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            content = soup.get_text()[:512]  # Truncate to avoid token limits\n",
    "        except:\n",
    "            return {\n",
    "                \"score\": 0.0,\n",
    "                \"explanation\": \"Failed to fetch URL content\"\n",
    "            }\n",
    "\n",
    "        # Get model prediction\n",
    "        try:\n",
    "            inputs = tokenizer(content, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                model_score = predictions[0][1].item()  # Assuming 1 is the \"reliable\" class\n",
    "        except:\n",
    "            return {\n",
    "                \"score\": 0.0,\n",
    "                \"explanation\": \"Failed to evaluate content with model\"\n",
    "            }\n",
    "\n",
    "        final_score = (0.7 * base_score) + (0.3 * model_score)  # Direct float multiplication\n",
    "        \n",
    "        return {\n",
    "            \"score\": final_score,\n",
    "            \"explanation\": f\"Base confidence: {base_score:.2f} | ML model confidence: {model_score:.2f}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"explanation\": f\"Error evaluating URL: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fact_check(url: str) -> Dict[str, Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Basic fact-checking evaluation based on known fact-checking sources and scientific journals.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains 'fact_check_score' (float) and 'explanation' (str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the URL\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc.lower()\n",
    "        \n",
    "        # Remove 'www.' if present\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "            \n",
    "        # Define authoritative sources and their scores\n",
    "        authoritative_sources = {\n",
    "            # Government Health Organizations\n",
    "            'ncbi.nlm.nih.gov': 0.95,    # National Center for Biotechnology Information\n",
    "            'nih.gov': 0.95,             # National Institutes of Health\n",
    "            'cdc.gov': 0.95,             # Centers for Disease Control\n",
    "            'fda.gov': 0.95,             # Food and Drug Administration\n",
    "            'who.int': 0.95,             # World Health Organization\n",
    "            \n",
    "            # Medical Research Institutions\n",
    "            'mayoclinic.org': 0.9,       # Mayo Clinic\n",
    "            'hopkinsmedicine.org': 0.9,   # Johns Hopkins Medicine\n",
    "            'medlineplus.gov': 0.9,       # MedlinePlus (NIH)\n",
    "            \n",
    "            # Scientific Journals\n",
    "            'nature.com': 0.9,\n",
    "            'science.org': 0.9,\n",
    "            'thelancet.com': 0.9,\n",
    "            'nejm.org': 0.9,             # New England Journal of Medicine\n",
    "            'jamanetwork.com': 0.9,      # Journal of American Medical Association\n",
    "            \n",
    "            # Fact-Checking Organizations\n",
    "            'cochrane.org': 0.9,         # Cochrane Reviews\n",
    "            'factcheck.org': 0.85,\n",
    "            'snopes.com': 0.85,\n",
    "            'reuters.com': 0.85,\n",
    "            'apnews.com': 0.85\n",
    "        }\n",
    "        \n",
    "        # Basic scoring logic\n",
    "        score = 0.5  # Default score\n",
    "        explanation = []\n",
    "        \n",
    "        # Check if it's a known authoritative source\n",
    "        for source, source_score in authoritative_sources.items():\n",
    "            if domain == source or domain.endswith('.' + source):\n",
    "                score = source_score\n",
    "                explanation.append(f\"Recognized authoritative source ({source})\")\n",
    "                break\n",
    "                \n",
    "        # Additional score for .gov domains not in our list\n",
    "        if domain.endswith('.gov') and score == 0.5:\n",
    "            score = 0.8\n",
    "            explanation.append(\"Government domain\")\n",
    "            \n",
    "        # Additional score for academic institutions\n",
    "        if domain.endswith('.edu'):\n",
    "            score = max(score, 0.8)\n",
    "            explanation.append(\"Academic institution\")\n",
    "            \n",
    "        # Check for scientific article indicators in URL\n",
    "        path = parsed_url.path.lower()\n",
    "        if any(x in path for x in ['/article/', '/research/', '/study/', '/paper/']):\n",
    "            score = min(score + 0.05, 1.0)\n",
    "            explanation.append(\"Scientific article indicators\")\n",
    "            \n",
    "        return {\n",
    "            \"fact_check_score\": score,\n",
    "            \"explanation\": \" | \".join(explanation) if explanation else \"Basic fact-check evaluation\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"fact_check_score\": 0.0,\n",
    "            \"explanation\": f\"Error evaluating URL: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(url: str, user_question: str, api_key: str) -> Dict[str, Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Evaluates the relevance of a URL based on NewsAPI article matching.\n",
    "    Args:\n",
    "    url (str): The URL to evaluate\n",
    "    user_question (str): The user's question for relevance matching\n",
    "    api_key (str): NewsAPI key for querying articles\n",
    "    Returns:\n",
    "    dict: Contains 'relevance_score' (float) and 'explanation' (str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize NewsAPI client\n",
    "        newsapi = NewsApiClient(api_key=api_key)\n",
    "        \n",
    "        # Parse the URL domain\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc.lower()\n",
    "        \n",
    "        # Extract base domain (remove www. and subdomains)\n",
    "        import tldextract\n",
    "        extracted = tldextract.extract(url)\n",
    "        base_domain = f\"{extracted.domain}.{extracted.suffix}\"\n",
    "        \n",
    "        # Initialize scoring\n",
    "        score = 0.5  # Default score\n",
    "        explanation = []\n",
    "        \n",
    "        # Search for articles related to the user's question\n",
    "        articles = newsapi.get_everything(\n",
    "            q=user_question,\n",
    "            domains=base_domain,\n",
    "            language='en',\n",
    "            sort_by='relevancy',\n",
    "            page_size=10\n",
    "        )\n",
    "        \n",
    "        # Check if articles were found\n",
    "        if articles['status'] == 'ok':\n",
    "            total_results = articles['totalResults']\n",
    "            if total_results > 0:\n",
    "                # Increase score based on number of articles\n",
    "                score += min(0.4, 0.1 * total_results)\n",
    "                explanation.append(f\"Found {total_results} relevant articles\")\n",
    "                \n",
    "                # Look for exact URL match\n",
    "                for article in articles['articles']:\n",
    "                    if url in article.get('url', ''):\n",
    "                        score = min(1.0, score + 0.3)\n",
    "                        explanation.append(\"Exact URL match found in news articles\")\n",
    "                        break\n",
    "            \n",
    "            # Bonus for scientific/research domains\n",
    "            scientific_domains = ['ncbi.nlm.nih.gov', 'nih.gov', 'nature.com', 'science.org']\n",
    "            if any(sci_domain in domain for sci_domain in scientific_domains):\n",
    "                score = max(score, 0.8)\n",
    "                explanation.append(\"Recognized scientific research domain\")\n",
    "        \n",
    "        # Cap the score at 1.0 and floor at 0.0\n",
    "        score = min(1.0, max(0.0, score))\n",
    "        \n",
    "        return {\n",
    "            \"relevance_score\": score,\n",
    "            \"explanation\": \" | \".join(explanation) if explanation else \"No direct relevance found via NewsAPI\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"relevance_score\": 0.0,\n",
    "            \"explanation\": f\"Error evaluating relevance: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bias(url: str, api_key: str) -> Dict[str, Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Evaluates bias using NewsAPI source data.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to evaluate\n",
    "        api_key (str): NewsAPI key\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains 'bias_score' (float) and 'explanation' (str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize NewsAPI client\n",
    "        newsapi = NewsApiClient(api_key=api_key)\n",
    "        \n",
    "        # Parse the URL\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc.lower()\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "            \n",
    "        # Get source information from NewsAPI\n",
    "        sources = newsapi.get_sources()\n",
    "        \n",
    "        score = 0.5  # Default score\n",
    "        explanation = []\n",
    "        \n",
    "        if sources['status'] == 'ok':\n",
    "            # Look for matching source\n",
    "            for source in sources['sources']:\n",
    "                source_url = urlparse(source['url']).netloc.lower()\n",
    "                if domain in source_url or source_url in domain:\n",
    "                    # Evaluate based on source category\n",
    "                    category = source['category']\n",
    "                    if category == 'science':\n",
    "                        score += 0.3\n",
    "                        explanation.append(\"Scientific news source\")\n",
    "                    elif category == 'technology':\n",
    "                        score += 0.2\n",
    "                        explanation.append(\"Technology news source\")\n",
    "                    elif category == 'health':\n",
    "                        score += 0.2\n",
    "                        explanation.append(\"Health news source\")\n",
    "                        \n",
    "                    # Consider language and country\n",
    "                    if source['language'] == 'en':\n",
    "                        score += 0.1\n",
    "                        explanation.append(\"English language source\")\n",
    "                        \n",
    "                    break\n",
    "        \n",
    "        # Add checks for .gov and .edu domains\n",
    "        if domain.endswith('.gov'):\n",
    "            score = max(score, 0.9)\n",
    "            explanation.append(\"Government domain\")\n",
    "        elif domain.endswith('.edu'):\n",
    "            score = max(score, 0.8)\n",
    "            explanation.append(\"Educational institution\")\n",
    "            \n",
    "        # Cap the score at 1.0\n",
    "        score = min(1.0, score)\n",
    "            \n",
    "        return {\n",
    "            \"bias_score\": score,\n",
    "            \"explanation\": \" | \".join(explanation) if explanation else \"Basic bias evaluation using NewsAPI\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"bias_score\": 0.0,\n",
    "            \"explanation\": f\"Error evaluating bias: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_citations(url: str) -> Dict[str, Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Evaluates citation count and reference quality from webpage content.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains 'citation_score' (float) and 'explanation' (str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Initialize counters and score\n",
    "        citation_count = 0\n",
    "        reference_count = 0\n",
    "        score = 0.5  # Default score\n",
    "        explanation = []\n",
    "        \n",
    "        # Find citations in different formats\n",
    "        # Look for <cite> tags\n",
    "        cite_tags = soup.find_all('cite')\n",
    "        citation_count += len(cite_tags)\n",
    "        \n",
    "        # Look for numbered references [1], [2], etc.\n",
    "        numbered_refs = re.findall(r'\\[\\d+\\]', response.text)\n",
    "        citation_count += len(numbered_refs)\n",
    "        \n",
    "        # Look for reference or bibliography section\n",
    "        ref_sections = soup.find_all(['div', 'section'], \n",
    "                                   class_=re.compile(r'reference|bibliography|citations', re.I))\n",
    "        \n",
    "        if ref_sections:\n",
    "            # Count references in these sections\n",
    "            for section in ref_sections:\n",
    "                # Count list items in reference sections\n",
    "                references = section.find_all('li')\n",
    "                reference_count += len(references)\n",
    "        \n",
    "        # Calculate score based on citations\n",
    "        total_citations = max(citation_count, reference_count)\n",
    "        \n",
    "        if total_citations > 0:\n",
    "            # Adjust score based on number of citations\n",
    "            if total_citations >= 50:\n",
    "                score = 0.9\n",
    "            elif total_citations >= 30:\n",
    "                score = 0.8\n",
    "            elif total_citations >= 15:\n",
    "                score = 0.7\n",
    "            elif total_citations >= 5:\n",
    "                score = 0.6\n",
    "            \n",
    "            explanation.append(f\"Found {total_citations} citations/references\")\n",
    "            \n",
    "        # Look for DOI references\n",
    "        doi_refs = re.findall(r'doi\\.org/\\d+\\.\\d+/\\S+', response.text)\n",
    "        if doi_refs:\n",
    "            score += 0.1  # Bonus for having DOI references\n",
    "            explanation.append(f\"Found {len(doi_refs)} DOI references\")\n",
    "            score = min(score, 1.0)  # Cap at 1.0\n",
    "            \n",
    "        return {\n",
    "            \"citation_score\": score,\n",
    "            \"explanation\": \" | \".join(explanation) if explanation else \"No citations found\",\n",
    "            \"citation_count\": total_citations\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"citation_score\": 0.0,\n",
    "            \"explanation\": f\"Error analyzing citations: {str(e)}\",\n",
    "            \"citation_count\": 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_scores(result_domain, result_relevance, result_fact_check, result_bias, result_citations):\n",
    "    \"\"\"\n",
    "    Aggregates individual scores into a final weighted score.\n",
    "    \"\"\"\n",
    "    # Define weights for each score component\n",
    "    weights = {\n",
    "        'domain': 0.2,\n",
    "        'relevance': 0.15,\n",
    "        'fact_check': 0.25,\n",
    "        'bias': 0.2,\n",
    "        'citations': 0.2\n",
    "    }\n",
    "    \n",
    "    # Extract scores\n",
    "    scores = {\n",
    "        'domain': result_domain.get('score', 0),\n",
    "        'relevance': result_relevance.get('relevance_score', 0),\n",
    "        'fact_check': result_fact_check.get('fact_check_score', 0),\n",
    "        'bias': result_bias.get('bias_score', 0),\n",
    "        'citations': result_citations.get('citation_score', 0)\n",
    "    }\n",
    "    \n",
    "    # Calculate weighted final score\n",
    "    final_score = sum(scores[key] * weights[key] for key in weights)\n",
    "    \n",
    "    return {\n",
    "        'final_score': round(final_score, 2),\n",
    "        'individual_scores': scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Scores:\n",
      "Domain Score: {'score': 0.6497420072555542, 'explanation': 'Base confidence: 0.50 | ML model confidence: 1.00'}\n",
      "Relevance Score: {'relevance_score': 0.8, 'explanation': 'Recognized scientific research domain'}\n",
      "Fact Check Score: {'fact_check_score': 0.95, 'explanation': 'Recognized authoritative source (ncbi.nlm.nih.gov)'}\n",
      "Bias Score: {'bias_score': 0.9, 'explanation': 'Government domain'}\n",
      "Citation Score: {'citation_score': 1.0, 'explanation': 'Found 80 citations/references | Found 80 DOI references', 'citation_count': 80}\n",
      "\n",
      "Final Aggregated Score:\n",
      "{'final_score': 0.87, 'individual_scores': {'domain': 0.6497420072555542, 'relevance': 0.8, 'fact_check': 0.95, 'bias': 0.9, 'citations': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main method to call all evaluation functions and return the scores based on user_question and url\n",
    "\"\"\"\n",
    "\n",
    "user_question = \"Are cigarettes addictive?\"\n",
    "url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2928221/\"\n",
    "\n",
    "newsapi_key = '5851f8384a5e4d93a44ac267f609dcd5'\n",
    "\n",
    "result_domain = evaluate_reference_credibility(url)\n",
    "result_relevance = evaluate_relevance(url, user_question, newsapi_key) \n",
    "result_fact_check = evaluate_fact_check(url)\n",
    "result_bias = evaluate_bias(url, newsapi_key)\n",
    "result_citations = evaluate_citations(url)\n",
    "\n",
    "# Print individual results\n",
    "print(\"Individual Scores:\")\n",
    "print(f\"Domain Score: {result_domain}\")\n",
    "print(f\"Relevance Score: {result_relevance}\")\n",
    "print(f\"Fact Check Score: {result_fact_check}\")\n",
    "print(f\"Bias Score: {result_bias}\")\n",
    "print(f\"Citation Score: {result_citations}\")\n",
    "\n",
    "# Calculate and print aggregate score\n",
    "final_results = aggregate_scores(result_domain, result_relevance, result_fact_check, result_bias, result_citations)\n",
    "print(\"\\nFinal Aggregated Score:\")\n",
    "print(final_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
